\chapter{Introduction}
\label{chap:intro}
% do we need to add TOC lines?

%\begin{figure}
%  \centering
%  \includegraphics[width=0.75\textwidth]{main/graphics/a_blowup.pdf}
%  \caption{This is a caption}
%\end{figure}

Creating 3D content plays an important role across industries such as gaming, augmented and virtual reality, and animation. These applications demand efficient, controllable processes for generating high-quality, editable 3D assets. In recent years, text-to-image diffusion models have significantly advanced 3D content creation, introducing new approaches for producing complex, detailed visual assets.

Early approaches to 3D content generation optimized 3D representations by leveraging 2D diffusion models~\cite{poole2022dreamfusion, wang2022scorejacobianchaininglifting}, producing high-quality assets but requiring time-consuming per-asset optimization. Recent techniques have adopted a two-stage process: first, synthesizing consistent multi-view images using 2D diffusion models~\cite{liu2023zero1to3, wang2023imagedream, shi2024mvdream, shi2023zero123singleimageconsistent, liu2023one2345, liu2023one2345++}, and second, performing 3D reconstruction from these multi-view images using feed-forward methods~\cite{instant3d2023, xu2024instantmesh}.
This approach accelerates 3D synthesis while maintaining high-quality output. However, by bypassing native 3D representations, it limits controllability and editability of the resulting assets and tends to produce visual artifacts such as the Janus problem and flat objects. Direct 3D generative models~\cite{jun2023shape, nichol2022pointe} offer greater control over creation and editing but are constrained by resolution limitations, resulting in lower-quality assets. This presents a trade-off between the quality of multi-view methods and the controllability of direct 3D generative models.



In this work, we address the quality gap between direct 3D generative models and those that reconstruct 3D objects from multi-view images. To bridge this gap, we introduce \emph{\ourname}, a multi-view to multi-view diffusion model that enhances objects generated by 3D generative models, refining geometric details and adding fine-grained texture features. The high-quality multi-view set produced by \ourname{} can be reconstructed into a 3D model using existing sparse-view feed-forward reconstruction methods~\cite{xu2024instantmesh, jin2024lvsmlargeviewsynthesis, hong2023lrm, zhuang2024gtr}.

\ourname{} is conditioned on a text prompt and operates on the multi-view set in parallel, sharing features across the generated views~\cite{shi2024mvdream, shi2023zero123singleimageconsistent}. We use Shap-E~\cite{jun2023shape} as our backbone generative model, a text-conditioned 3D latent diffusion model that operates on the parameters of implicit functions. Shap-E has been shown to offer rich generative capabilities~\cite{chen2023shapeditor, sella2024spicee}, but produces low-quality assets. To train \ourname, we use pairs of multi-view sets obtained by encoding high-quality 3D objects~\cite{Deitke_2023} into Shap-E's latent space and rendering them.


Our approach offers two key advantages. First, since we start from a coarsely valid 3D object, we inherit its plausible geometric structure, avoiding artifacts like the Janus problem that can occur in direct multi-view synthesis methods. Second, the 3D consistency of the input views facilitates output consistency, allowing the model to focus on generating the fine details necessary for high-quality results.

% The high-quality multi-view set produced by \ourname{} can be reconstructed into a 3D model using existing sparse-view feed-forward reconstruction methods~\cite{xu2024instantmesh, jin2024lvsmlargeviewsynthesis, hong2023lrm, zhuang2024gtr}. Furthermore, we demonstrate that the \ourname-generated multi-view set can serve as keyframes for a video generation model, enabling the creation of 360-degree dense views. 

Our method combines the controllability of native 3D generative models with the capabilities of 2D diffusion models in generating highly detailed images. Through extensive experiments, we demonstrate that \ourname{} outperforms existing enhancement methods in both quality and efficiency. We show various applications, including fast generation of high-quality 3D objects from text prompts and geometric controls~\cite{sella2024spicee}. Additionally, we show that \ourname{} enhances 3D asset editing capabilities.


