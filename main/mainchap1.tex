\chapter{A main chapter}
\label{chap:firstchap}
\input{figures/method}
\section{Method}
In this section, we present our approach to addressing the quality gap between direct 3D generative models and those that reconstruct 3D objects from multi-view images.
We focus on Shap-E~\cite{jun2023shape}, a 3D generative model, and introduce \emph{Sharp-It}, a multi-view-to-multi-view diffusion model that enhances 3D objects generated by Shap-E. We train Sharp-It to improve these objects by adding intricate appearance details and correcting geometric artifacts such as discontinuities and broken parts.
% We begin by providing the necessary background on Shap-E and Zero123++, the diffusion models upon which Sharp-It builds.
The process of generating a 3D model with Sharp-It is demonstrated in Figure~\ref{fig:method}.

\subsection{Preliminaries}

\paragraph{Shap-E} 
is a latent diffusion model specifically designed for generating 3D assets. As common in latent diffusion models~\cite{rombach2022highresolutionimagesynthesislatent}, Shap-E is trained in two stages. In the first stage, an encoder is trained to map 3D objects into a latent space. This latent space corresponds to the weight space of implicit functions that represent 3D shapes, with each shape represented as an element in $\mathbb{R}^{1024\times1024}$. The latent representation can be decoded using Signed Texture Field (STF) rendering, where it is treated as the weights of an implicit function.
In the second stage, a diffusion model is trained within the Shap-E latent space, allowing for conditioning on either text or images.

\paragraph{Zero123++}
is an image-conditioned diffusion model designed to generate 3D-consistent multi-view images from a single input view~\cite{shi2023zero123singleimageconsistent}
. It builds upon Stable Diffusion~\cite{rombach2022highresolutionimagesynthesislatent}, which is a latent diffusion model comprising a VAE and a UNet. The VAE encodes images into a resolution that is eight times smaller and consists of four channels, while the UNet serves as the diffusion model, operating on the four-channel latent codes.
Zero123++ is a fine-tuned version of Stable Diffusion that accepts an image as input. Given an input image, it produces a $3\times2$ grid of $320\times320$ pixel images, with six constant azimuth and elevation angles. 
Originally, Zero123++~\cite{shi2023zero123singleimageconsistent} was designed to generate this grid image with a grey background. In InstantMesh~\cite{xu2024instantmesh}, it was further fine-tuned to render a white background, addressing the issue of ``floaties''—particles floating through space during 2D-to-3D lifting.

\subsection{Dataset Construction} \label{sec:data}
We begin by constructing a paired dataset consisting of degraded Shap-E objects along with corresponding high-quality objects. Our key idea in constructing the dataset is that such pairs can be obtained by employing the encoder of Shap-E in conjunction with a high-quality 3D objects dataset.
Specifically, we utilize objects from Objaverse~\cite{deitke2023objaversexluniverse10m3d}, provided by~\cite{luo2023scalable, luo2024view}, and encode them using Shap-E's encoder. For each pair of an object from Objaverse and its encoded Shap-E latent code, we render a $3\times2$ grid from six predefined camera views, applying three HDR lighting conditions. Our experiments, detailed in the ablation studies, indicate that rendering each object under varying HDR lighting enhances the model’s performance.

We apply a filtering criteria on the resulting dataset.
We remove objects where the degraded Shap-E rendering is significantly different from the original object's rendering, indicating a failure of Shap-E's encoder. Additionally, we filter out objects that are too thin or do not match certain keywords based on their annotated captions. For each remaining object, we extract a caption using BLIP2~\cite{li2023blip2}.
Finally, we split the dataset into training and test sets, resulting in 180,000 objects for training and 6,000 for testing.


\subsection{Sharp-It}
Sharp-It is a multi-view to multi-view diffusion model designed to enhance low-quality multi-view images of 3D objects generated by Shap-E. It takes as input a set of multi-view images rendered from a low-quality 3D object, along with a textual prompt, and produces high-quality multi-view images with refined geometric details and textures that correspond to the input views.
% In practice, the multi-view image set is organized as a grid of $3\times2$ views, resulting in a total resolution of $960\times640$. Each view has its own azimuth and elevation angles, which remain fixed across different objects. 
An overview of a 3D generation pipeline with Sharp-It is shown in Figure~\ref{fig:method}.

\paragraph{Architecture}
The architecture of Sharp-It has two key requirements: generating multi-view image sets and incorporating input multi-view sets as conditions. We build on Zero123++~\cite{shi2023zero123singleimageconsistent, xu2024instantmesh}, which was fine-tuned for multi-view generation, where images are arranged in a $3\times2$ grid with a total resolution of $960\times640$ and fixed camera angles across objects.
%
To enable multi-view conditioning, we modify the architecture of Zero123++ by expanding the UNet input to 8 channels: 4 for latent noise and 4 for the VAE-encoded Shap-E multi-view images. This design lets our model leverage the coarse geometry from the input views to achieve better 3D consistency, and is inspired by image editing techniques that fine-tune diffusion models to accept an image as input and modify specific parts while preserving others~\cite{brooks2022instructpix2pix, rombach2022highresolutionimagesynthesislatent, yang2022paint}. Unlike these approaches, which operate on a single image, our model learns to enhance the input views in a 3D consistent manner. 
Furthermore, we replace Zero123++'s image embedding with text prompts in the cross-attention layers, enabling better enhancement control and appearance editing capabilities (Section~\ref{sec:app-edit}).


The model we build on consists of self-attention layers, which play an important role in facilitating the consistency of our produced multi-view set~\cite{shi2024mvdream, wang2023imagedream}. Since our model operates on a multi-view image grid, these layers can be seen as an application of cross-view attention between the different views. Similarly to previous works~\cite{wang2023imagedream, shi2024mvdream, shi2023zero123singleimageconsistent}, this allows our model to simultaneously refine corresponding points across different views by learning the correspondences between them. 
We visualize the learned correspondences in Figure~\ref{fig:cross-view-attention}. The figure displays self-attention maps for a query point marked by a red dot in the leftmost image. The results demonstrate that this point on the wheel receives the highest attention weight across different views. Additionally, the attention mechanism identifies semantically similar points -- notably, other wheels of the car.

\input{figures/comparisons}
\input{tables/experiments}

\input{figures/comparisons}
\input{tables/experiments}

\section{Experiments}
In this section, we evaluate the performance of Sharp-It in enhancing 3D objects generated by Shap-E. We compare our method against several baselines, conduct ablation studies to analyze the impact of different components, and demonstrate applications in 3D generation and editing.

\subsection{Qualitative and Quantitative Comparison}


\paragraph{Baselines}

We compare Sharp-It with various enhancement baselines. First, we compare against an SDS-based approach that was used in Spice-E~\cite{sella2024spicee}. Specifically, we use GaussianDreamer~\cite{yi2024gaussiandreamerfastgenerationtext}, initializing Gaussian Splatting with Shap-E's shape output and optimizing it with a text-to-image model. 
To make refining the thousands of objects in our test set practical, we limit the optimization time to six minutes per object.
We also include MVEdit~\cite{mvedit2024}, a multi-view editing method that  refines coarse 3D shapes.

\vspace{-10pt}
Another set of baselines combines multi-view image generation with SDEdit~\cite{meng2022sdedit}. Here, we render Shap-E’s shape into a multi-view image set and apply simultaneous edits across views to ensure consistency. Using SDEdit with a strength of 0.4, we add noise to each multi-view image and denoise it. For multi-view generation, we use both MVDream~\cite{shi2024mvdream}, which is guided by a text prompt, and Zero123++~\cite{shi2023zero123singleimageconsistent}, which is conditioned on an image rather than text.
With Zero123++, we test three configurations:
(i) Zero123++ w/ SDEdit (R), where we apply SDXL Refiner model (default strength value)~\cite{podell2023sdxlimprovinglatentdiffusion} on the frontal view of the object, and use this edited view to guide SDEdit across the multi-view set, (ii) Zero123++ w/ SDEdit (C), where SDEdit with Stable Diffusion (strength 0.75) is applied on the object's frontal view which is used to guide the multi-view SDEdit; and (iii) Zero123++ w/ SDEdit (U), where no image condition is used.
 

\vspace{-10pt}
\paragraph{Dataset}
We conduct our quantitative experiments on our test set described in Section~\ref{sec:data}.
% a subset of Objaverse~\cite{deitke2023objaversexluniverse10m3d} that was held out during the training of Sharp-It. Each object in this subset is first encoded into Shap-E's latent space and then enhanced using each method. 
To demonstrate our method's generalization capability, we also present results on objects generated directly by Shap-E from text prompts.

\vspace{-12pt}
\paragraph{Metrics}
To assess enhancement performance, we evaluate the image quality produced by each method. Specifically, we compute the FID~\cite{parmar2021cleanfid, heusel2017gans} between the enhanced views and our test set. Additionally, we evaluate the semantic and visual similarity of the enhanced shape with the ground-truth using CLIP~\cite{radford2021learning} and DINO~\cite{caron2021emerging} image encoders.

\vspace{-12pt}
\paragraph{Results}
We present a qualitative comparison in Figure~\ref{fig:mv2mv-comparisons}. The avocado couch and the tiki mask were generated by Shap-E, while the bust is an encoded object from the test set. As shown, our method preserves the objects' coarse details while producing high-quality, detailed enhancements. In contrast, other methods diverge significantly from the original objects and yield less realistic results. For instance, our method accurately generates a leather-like texture, whereas other methods struggle to enhance the flat appearance of the Shap-E-generated couch.
Our approach also maintains color consistency with the input shapes. This is evident in the couch's back section, where brown and green tints are faithfully preserved, as well as in the bust and tiki mask. Our method generates significantly more fine details compared to other methods, particularly visible in the intricate features of the bust and the textural elements of the mask. 
Results for the objects lifted to 3D are provided in the supplementary materials.
% Additional lifted results for each method are provided in the supplementary materials.

Quantitative comparisons are shown in Table~\ref{tab:performance_comparison}. Our method achieves the lowest FID, confirming it produces the highest-quality results. Furthermore, our method shows the best alignment with the ground-truth object, as indicated by the CLIP and DINO similarity metrics, The runtime of our method is comparable to or faster than other methods.



\subsection{Ablation Studies}


\input{figures/ablations_t}

We conduct ablation studies to assess the contributions of different components in Sharp-It. 
First, we examine the importance of using an input text prompt, which is not utilized in Zero123++~\cite{shi2023zero123singleimageconsistent, xu2024instantmesh}, the model upon which we build.
Second, we explore the effect of constructing the dataset with diverse lighting conditions, rather than using a single type of lighting.
% Specifically, we examine the importance of using an input text prompt and the effect of constructing the dataset with diverse lighting conditions. 

For this analysis, we train our method without each component individually and present the results in Figure~\ref{fig:ablation}.
For computational efficiency, ablation models (including the full method one) were trained for 400,000 steps, fewer than in our main experiments.
\input{tables/ablations}
In the first column, we show a shape generated by Shap-E, which serves as input to the models in the subsequent rows. In the second column, we omit the text prompt, instead using an empty prompt. As shown, without a guiding text prompt, the results lack detail and fail to achieve a metallic appearance. In the third column, we construct the dataset with a single lighting condition, leading to a notable enhancement of the input shape but still lacking finer details. The last column displays results from our full model, which successfully achieves a metallic look and produces highly detailed shapes. Importantly, our results are consistent with the coarse details of the input shape. The lifted object of this example is included in the supplementary materials.
We also quantitatively evaluate each configuration using the metrics described in the previous section. Results are provided in Table~\ref{tab:ablation}. While all configurations are comparable aligned with the ground truth object, the full model achieves the highest image quality.


\input{figures/generation_results}

\section{Applications}
We now demonstrate the various applications enabled by Sharp-It. Specifically, we show how Sharp-It bridges the quality gap between Shap-E and multi-view-based 3D generation methods, while supporting diverse 3D generative applications.


\vspace{-16pt}
\paragraph{Text-to-3D Generation}
As discussed in previous sections, a common approach for text-to-3D synthesis involves first generating a nearly consistent multi-view image set~\cite{shi2024mvdream, wang2023imagedream, shi2023zero123singleimageconsistent}, followed by a sparse-view 3D reconstruction method~\cite{xu2024instantmesh}. By combining Shap-E with Sharp-It, we achieve text-to-multi-view image synthesis, enabling a complete pipeline for 3D generation (Figure~\ref{fig:method}).
In Figure~\ref{fig:text-to-3d}, we compare text-to-multi-view results from Zero123++ and our method. Our method achieves results comparable to Zero123++ in terms of image quality, while demonstrating superior geometric details. The advantage of using Shap-E as a first stage is particularly evident in the tank and Gundam examples. The tank produced by Zero123++ suffers from a Janus problem, where wheels incorrectly appear on two adjacent sides. Moreover, Zero123++ generates a flat Gundam figure, lacking geometric details. These results demonstrate the effectiveness of our pipeline: using a 3D-aware model to generate a coarse 3D object, followed by refinement using an image diffusion model.


\vspace{-12pt}
\paragraph{3D Object Editing}

\input{figures/edits}
Shap-E provides a latent space in which semantic manipulation of 3D shapes can be performed while maintaining 3D consistency. Our method enhances these edited shapes to achieve high-quality objects. We demonstrate our method's application with two different editing techniques.
First, we use Shap-Editor~\cite{chen2023shapeditor}, which trains a model that takes a Shap-E latent code and a text instruction to produce a new latent code corresponding to the edited shape. While this method enables diverse and fast editing of 3D shapes, it is bounded by Shap-E's quality limitations. Sharp-It addresses this limitation by enhancing the results produced by Shap-Editor.
Second, inspired by diffusion-based editing methods for images, we apply a technique that enables shape editing with Shap-E. Specifically, we apply Edit-Friendly DDPM Inversion~\cite{huberman2024edit} with Shap-E instead of an image diffusion model.

Results for both methods are shown in Figure~\ref{fig:edits}. By combining Sharp-It with existing editing methods, we enable various types of high-quality 3D edits. These include changing an object's shape (demonstrated by transforming a car into an SUV), adding decorations (shown with the Christmas lamp), and modifying colors (as seen with the golden lamp and turquoise car).


\vspace{-16pt}
\paragraph{Appearance Editing} \label{sec:app-edit}
Sharp-It enables detailed control over the appearance of a degraded shape using text prompts. Specifically, at inference time, we can apply a different prompt from the one used during the generation of the Shap-E object, facilitating appearance editing that accurately preserves the original shape. Examples of this application are presented in Figures~\ref{fig:teaser} and \ref{fig:edits-appearance}.

% \input{figures/edits_appearance}
\input{figures/controlled_generation}

\vspace{-14pt}
\paragraph{Controlled Generation}
Previous works have trained models that provide coarse geometric control over objects generated by Shap-E. Similar to Shap-E-based editing methods, such approaches are bounded by Shap-E's quality limitations. We demonstrate high-quality controlled 3D generation by applying our method to shapes generated with Spice-E~\cite{sella2024spicee}.
We show these results in Figure~\ref{fig:controlled-generation}. Our method significantly improves both object textures and fine geometric details. Notably, while Spice-E~\cite{sella2024spicee} uses an SDS-based approach as a refinement step, which impacts its runtime, our method achieves comparable or better results while running an order of magnitude faster.

% \input{figures/controlled_generation}
\input{figures/edits_appearance}

\vspace{-3pt}
